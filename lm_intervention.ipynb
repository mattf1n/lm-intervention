{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/boknilev/lm-intervention/blob/master/lm_intervention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mpVWB0MeXxbT",
    "outputId": "b0737332-f626-47f8-be32-09033770258c"
   },
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert\n",
    "#!pip install spacy ftfy==4.4.3\n",
    "#!python -m spacy download en\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Intervention, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcrFo1lMfpci"
   },
   "outputs": [],
   "source": [
    "# Currently not implemented/needed functions. \n",
    "def plot_log_probs(layer_to_candidate1_log_probs, layer_to_candidate2_log_probs):\n",
    "    \n",
    "    raise NotImplementedError\n",
    "        \n",
    "def print_neuron_hook(module, input, output, position, neuron):\n",
    "        #print(output.shape) \n",
    "        print(output[0][position][neuron])\n",
    "        \n",
    "def print_all_hook(module, input, output, position):\n",
    "    #print(output.shape) \n",
    "    print(output[0][position])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping for all the experiments\n",
    "\n",
    "ToDo: actually run all of them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test experiment\n",
    "'''\n",
    "base_sentence = \"The {} said that\"\n",
    "biased_word = \"teacher\"\n",
    "intervention = Intervention(\n",
    "        tokenizer,\n",
    "        base_sentence,\n",
    "        [biased_word, \"man\", \"woman\"],\n",
    "        [\"he\", \"she\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential other sentences we could check. They could amplify the effect\n",
    "# To Do: make different interventions with each after finalizing list.\n",
    "base_sentence = \"The {} said that\"\n",
    "base_sentence = \"The {} yelled that\"\n",
    "base_sentence = \"The {} whispered that\"\n",
    "base_sentence = \"The {} wanted that\"\n",
    "base_sentence = \"The {} desired that\"\n",
    "base_sentence = \"The {} wished that\"\n",
    "\n",
    "base_sentence = \"A {} caught the ball.\"\n",
    "base_sentence = \"A {} cooked the dinner.\"\n",
    "base_sentence = \"The {} baked the cake.\"\n",
    "base_sentence = \"A {} raised the gun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: does not include teacher\n",
    "profession_interventions = []\n",
    "with open('professions.json', 'r') as f:\n",
    "    for l in f: \n",
    "        # there is only one line that eval's to an array\n",
    "        for j in eval(l):\n",
    "            biased_word = j[0]\n",
    "            profession_interventions.append(\n",
    "                Intervention(\n",
    "                    tokenizer,\n",
    "                    \"The {} said that\",\n",
    "                    [biased_word, \"man\", \"woman\"],\n",
    "                    [\"he\", \"she\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Need to double check that the sentences all make sense\n",
    "male_interventions = []\n",
    "with open('male_word.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        # Strip off the \\n\n",
    "        biased_word = l[:-1]\n",
    "        male_interventions.append(\n",
    "                Intervention(\n",
    "                    tokenizer,\n",
    "                    \"The {} said that\",\n",
    "                    [biased_word, \"man\", \"woman\"],\n",
    "                    [\"he\", \"she\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Need to double check that the sentences all make sense\n",
    "female_interventions = []\n",
    "with open('female_word.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        # Strip off the \\n\n",
    "        biased_word = l[:-1]\n",
    "        female_interventions.append(\n",
    "                Intervention(\n",
    "                    tokenizer,\n",
    "                    \"The {} said that\",\n",
    "                    [biased_word, \"man\", \"woman\"],\n",
    "                    [\"he\", \"she\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Test Experiment Run\n",
    "\n",
    "ToDo: add eval/vis of result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE8stKqggffZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Code draws on https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py \"\"\"        \n",
    "c1, c1_probs, c2, c2_probs = model.neuron_intervention_experiment(intervention)\n",
    "# TODO: we need to look at the log prob distribution over the candidates and how that changes by intervention  \n",
    "'''\n",
    "Report aggregate\n",
    "'''\n",
    "print('more probable candidate per layer, across all neurons in the layer')\n",
    "print('candidate1:', intervention.candidates[0], layer_to_candidate1)\n",
    "print('candidate2:', intervention.candidates[1], layer_to_candidate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lm-intervention.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
