{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/boknilev/lm-intervention/blob/master/lm_intervention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mpVWB0MeXxbT",
    "outputId": "b0737332-f626-47f8-be32-09033770258c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112024ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install pytorch-pretrained-bert\n",
    "#!pip install spacy ftfy==4.4.3\n",
    "#!python -m spacy download en\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from functools import partial \n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "class Intervention():\n",
    "    '''\n",
    "    Wrapper for all the possible interventions\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 base_string:str, \n",
    "                 substitutes:list, \n",
    "                 candidates:list):\n",
    "        super()\n",
    "        self.enc = tokenizer\n",
    "        # All the initial strings\n",
    "        # First item should be neutral, others tainted\n",
    "        self.base_strings = [base_string.format(s) \n",
    "                             for s in substitutes]\n",
    "        # Tokenized bases\n",
    "        self.base_strings_tok = [self._to_batch(s)\n",
    "                                 for s in self.base_strings]\n",
    "        # Where to intervene\n",
    "        self.position = base_string.split().index('{}')\n",
    "        \n",
    "        # How to extend the string\n",
    "        self.candidates = ['Ġ' + c for c in candidates]\n",
    "        # tokenized candidates\n",
    "        self.candidates_tok = [self.enc.convert_tokens_to_ids(c) \n",
    "                               for c in self.candidates]\n",
    "        \n",
    "    def _to_batch(self, txt):\n",
    "        encoded = self.enc.encode(txt) \n",
    "        return torch.tensor(encoded, dtype=torch.long)\\\n",
    "                    .unsqueeze(0)\\\n",
    "                    .repeat(1, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    '''\n",
    "    Wrapper for all model logic\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super()\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Options\n",
    "        self.top_k = 5\n",
    "        # 12 for GPT-2\n",
    "        self.num_layers = len(self.model.transformer.h)\n",
    "        # 768 for GPT-2 \n",
    "        self.num_neurons = self.model.transformer.wte.weight.shape[1] \n",
    "        \n",
    "        # multiplier for intervention; needs to be pretty large (~100) to see a change\n",
    "        # TODO: plot the intervention results (how many neurons are flipped) for different alphas\n",
    "        self.alpha = 500\n",
    "        \n",
    "    def get_representations(self, context, position):\n",
    "        # Hook for saving the representation\n",
    "        def extract_representation_hook(module, input, output, position, representations, layer):\n",
    "            representations[layer] = output[0][position]\n",
    "        handles = []\n",
    "        representation = {}\n",
    "        with torch.no_grad():\n",
    "            # construct all the hooks\n",
    "            for layer in range(self.num_layers):\n",
    "                handles.append(self.model.transformer.h[0]\\\n",
    "                                   .mlp.register_forward_hook(\n",
    "                    partial(extract_representation_hook, \n",
    "                            position=position, \n",
    "                            representations=representation, \n",
    "                            layer=layer)))\n",
    "            logits, past = model.model(context)\n",
    "            for h in handles:\n",
    "                h.remove()\n",
    "        print(representation[0][:5])\n",
    "        return representation\n",
    "    \n",
    "    def get_probabilities_for_examples(self, context, outputs):\n",
    "        logits, past = self.model(context)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "#         print(probs.shape)\n",
    "#         print(outputs)\n",
    "        return probs[0][outputs]\n",
    "\n",
    "    def intervene_for_examples(self, \n",
    "                               context, \n",
    "                               outputs, \n",
    "                               repr_difference, \n",
    "                               layer, \n",
    "                               neuron, \n",
    "                               position):\n",
    "        # Hook for changing representation during forward pass\n",
    "        def intervention_hook(module, input, output, position, neuron, intervention):\n",
    "            output[0][position][neuron] += intervention\n",
    "        \n",
    "        intervention_rep = self.alpha * repr_difference[layer][neuron]\n",
    "        mlp_intervention_handle = self.model.transformer.h[layer]\\\n",
    "                                       .mlp.register_forward_hook(\n",
    "            partial(intervention_hook, \n",
    "                    position=position, \n",
    "                    neuron=neuron, \n",
    "                    intervention=intervention_rep))\n",
    "        new_probabilities = self.get_probabilities_for_examples(\n",
    "            context, \n",
    "            outputs)\n",
    "        mlp_intervention_handle.remove()\n",
    "        return new_probabilities\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcrFo1lMfpci"
   },
   "outputs": [],
   "source": [
    "# TODO: plot the log probs nicely \n",
    "def plot_log_probs(layer_to_candidate1_log_probs, layer_to_candidate2_log_probs):\n",
    "    \n",
    "    raise NotImplementedError\n",
    "        \n",
    "def print_neuron_hook(module, input, output, position, neuron):\n",
    "        #print(output.shape) \n",
    "        print(output[0][position][neuron])\n",
    "        \n",
    "def print_all_hook(module, input, output, position):\n",
    "    #print(output.shape) \n",
    "    print(output[0][position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Bookkeeping for experiments\n",
    "'''\n",
    "\n",
    "intervention = Intervention(\n",
    "        \"The {} said that\",\n",
    "        [\"teacher\", \"man\", \"woman\"],\n",
    "        [\"he\", \"she\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To Do: actually run all of them. note: does not include teacher\n",
    "'''\n",
    "\n",
    "profession_interventions = []\n",
    "with open('professions.json', 'r') as f:\n",
    "    for l in f: \n",
    "        # there is only one line that eval's to an array\n",
    "        for j in eval(l):\n",
    "            profession = j[0]\n",
    "            profession_interventions.append(\n",
    "                Intervention(\n",
    "                    \"The {} said that\",\n",
    "                    [profession, \"man\", \"woman\"],\n",
    "                    [\"he\", \"she\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE8stKqggffZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9688, -0.0767, -1.3549, -0.5847, -1.2433])\n",
      "tensor([-0.4244,  0.8223, -1.8646, -0.8140, -1.0686])\n",
      "Base case: The teacher said that ____\n",
      "Ġhe: 9.81%\n",
      "Ġshe: 12.17%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d9ff5a10a24b67bd2ca8ed25fa0e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='layers', max=12, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be054a831b1941ba98b3b99e0bff4d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96186ce1d6e34dc8a6ce1785e7cb109b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03652a9b51f477eaa4a6390bbba1de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19cd7f44bc449fd9a3240ff8e0630ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be632948d1a46929d69092cb6c53b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf675a91a09423da37c4cc696c51dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865afa94ad7c46f488ad823883316f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb30a1000a455086ecd6f39dd0aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103fabafbd254465a00a3e5fc7f570f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607dc4d3812f4992ba69d41a1b29483a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bc72669c654f2996d69dfeb685aad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2c6e3bab7048369da894840741c074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='neurons', max=768, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_to_candidate1, layer_to_candidate2 = Counter(), Counter()\n",
    "layer_to_candidate1_probs, layer_to_candidate2_probs = defaultdict(list), defaultdict(list)\n",
    "\n",
    "\n",
    "\"\"\" Code draws on https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py \"\"\"        \n",
    "with torch.no_grad():\n",
    "    '''\n",
    "    Compute representations for gendered terms\n",
    "    ''' \n",
    "    man_representations = model_repr = model.get_representations(\n",
    "        intervention.base_strings_tok[1], \n",
    "        intervention.position)\n",
    "    woman_representations = model.get_representations(\n",
    "        intervention.base_strings_tok[2], \n",
    "        intervention.position)\n",
    "    representation_difference = {k: v - woman_representations[k] \n",
    "                                 for k,v in man_representations.items()}\n",
    "    '''\n",
    "    Now intervening on potentially biased example\n",
    "    '''\n",
    "    context = intervention.base_strings_tok[0]\n",
    "    '''\n",
    "    Probabilities without intervention (Base case)\n",
    "    '''\n",
    "    base_probs = model.get_probabilities_for_examples(\n",
    "        context, \n",
    "        intervention.candidates_tok)\n",
    "    print(\"Base case: {} ____\".format(intervention.base_strings[0]))\n",
    "    for token, prob in zip(intervention.candidates, base_probs):\n",
    "        print(\"{}: {:.2f}%\".format(token, prob*100))\n",
    "\n",
    "    '''\n",
    "    Intervene at every possible neuron\n",
    "    '''\n",
    "    for layer in tqdm_notebook(range(model.num_layers), desc='layers'):\n",
    "        for neuron in tqdm_notebook(range(model.num_neurons), desc='neurons'):\n",
    "            candidate1_prob, candidate2_prob = model.intervene_for_examples(\n",
    "                context=context, \n",
    "                outputs=intervention.candidates_tok, \n",
    "                repr_difference=representation_difference, \n",
    "                layer=layer, \n",
    "                neuron=neuron, \n",
    "                position=intervention.position)\n",
    "\n",
    "            layer_to_candidate1_probs[layer].append(candidate1_prob)\n",
    "            layer_to_candidate2_probs[layer].append(candidate2_prob)\n",
    "            if candidate1_prob > candidate2_prob:\n",
    "                layer_to_candidate1[layer] += 1\n",
    "            else:\n",
    "                layer_to_candidate2[layer] += 1\n",
    "    \n",
    "    # TODO: we need to look at the log prob distribution over the candidates and how that changes by intervention  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more probable candidate per layer, across all neurons in the layer\n",
      "candidate1: Ġhe Counter({0: 672, 2: 668, 1: 664, 3: 633, 4: 569, 5: 561, 6: 428, 7: 375, 8: 359, 9: 158, 10: 37})\n",
      "candidate2: Ġshe Counter({11: 768, 10: 731, 9: 610, 8: 409, 7: 393, 6: 340, 5: 207, 4: 199, 3: 135, 1: 104, 2: 100, 0: 96})\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Report aggregate\n",
    "'''\n",
    "print('more probable candidate per layer, across all neurons in the layer')\n",
    "print('candidate1:', intervention.candidates[0], layer_to_candidate1)\n",
    "print('candidate2:', intervention.candidates[1], layer_to_candidate2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lm-intervention.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
