# lm-intervention
Intervention experiments in language models

## Requirements

`transformers==2.7`

## Templates

Templates are based on [Lakretz et al. (2019)](https://github.com/FAIRNS/Number_and_syntax_units_in_LSTM_LMs) 

| Structure | Template |
| --- | --- |
| Simple | The [noun] [verb] |
| 1 distractor | The [noun] [adv1] [verb] |
| 2 distractor | The [noun] [adv1] and [adv2] [verb] | 
| Across prepositional phrase | The [noun] [prep] the [prepnoun] [verb] |
| Across relative clause | The [noun] the [noun2] [verb2] [verb] | 
| Within relative clause | The [noun2] the [noun] [verb] |

Drop-in values for [noun], [verb], etc., can be found in `vocab/wordlists`.

## Experiments

### Neuron interventions

To run the experiment, run 
```
python neuron_experiment_multiple_templates_num_agreement.py \
<model> <device> <out_dir> <random_weights> <attractor> <seed> <examples>`
```
from the project directory.

```
model: Which model to run experiments on
  distilgpt2 | gpt2 | gpt2-medium | gpt2-large | gpt2-xl 
| transfo-xl-wt103 | xlnet-base-cased

device: Device for pytorch
  cpu | cuda 

out_dir: Output directory for the results

random_weights: Whether to use a randomly initialized (untrained) model 
  random | not_random 

attractor: Which syntactic structure to use
  none | singular | plural | rc_singular | rc_plural | rc_singular_no_that
| rc_plural_no_that | within_rc_singular | within_rc_plural   
| within_rc_singular_no_that | within_rc_plural_no_that | distractor 
| distractor_1

seed: Integer random seed for reproducible results

examples: Integer number of examples to use from the template. 
  0 uses all examples.
```

Running this experiment is very computationally expensive, and can take hours on a GPU. The outputs also require gigabytes of space for the largest models.

Results are output to `results/<date>_neuron_intervention/<attractor>_<direct|indirect>_<model>.csv`
Once all outputs are obtained, running `python make_feathers.py <out_dir>/results` space-efficient `.feather` files from the `.csv` files.

### Attention head interventions

Attention experiments are run via `attention_intervention_structural.py` script which take the same command line arguments as the `neuron_experiment_multiple_templates_num_agreement.py` script.

Outputs are `.json` files in the `attention_results` directory.

## Generating figures

The majority of figures from the paper are generated in `plots.ipynb`.
Attention head figures are generated by the `attention_figures*.py` scripts.

## Other Papers / Resources

Code is based on [Vig et al. (2020)](https://github.com/sebastianGehrmann/CausalMediationAnalysis)

Templates are based on [Lakretz et al. (2019)](https://github.com/FAIRNS/Number_and_syntax_units_in_LSTM_LMs) 
