You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
words:   0%|          | 0/320 [00:00<?, ?it/s]words:   0%|          | 1/320 [00:16<1:29:36, 16.85s/it]words:   1%|          | 2/320 [00:27<1:18:41, 14.85s/it]words:   1%|          | 3/320 [00:37<1:11:10, 13.47s/it]words:   1%|▏         | 4/320 [00:47<1:05:45, 12.49s/it]words:   2%|▏         | 5/320 [00:57<1:01:56, 11.80s/it]words:   2%|▏         | 6/320 [01:07<59:13, 11.32s/it]  words:   2%|▏         | 7/320 [01:18<57:20, 10.99s/it]words:   2%|▎         | 8/320 [01:28<55:55, 10.76s/it]words:   3%|▎         | 9/320 [01:38<54:56, 10.60s/it]words:   3%|▎         | 10/320 [01:48<54:17, 10.51s/it]words:   3%|▎         | 11/320 [01:59<53:37, 10.41s/it]words:   4%|▍         | 12/320 [02:09<53:07, 10.35s/it]words:   4%|▍         | 13/320 [02:19<52:43, 10.30s/it]words:   4%|▍         | 14/320 [02:29<52:47, 10.35s/it]words:   5%|▍         | 15/320 [02:40<52:25, 10.31s/it]words:   5%|▌         | 16/320 [02:50<52:05, 10.28s/it]words:   5%|▌         | 17/320 [03:00<51:48, 10.26s/it]words:   6%|▌         | 18/320 [03:10<51:34, 10.25s/it]words:   6%|▌         | 19/320 [03:20<51:19, 10.23s/it]words:   6%|▋         | 20/320 [03:31<51:16, 10.25s/it]words:   7%|▋         | 21/320 [03:41<51:01, 10.24s/it]words:   7%|▋         | 22/320 [03:51<50:47, 10.23s/it]words:   7%|▋         | 23/320 [04:01<50:35, 10.22s/it]words:   8%|▊         | 24/320 [04:12<50:23, 10.22s/it]words:   8%|▊         | 25/320 [04:22<50:12, 10.21s/it]words:   8%|▊         | 26/320 [04:32<50:01, 10.21s/it]words:   8%|▊         | 27/320 [04:42<50:00, 10.24s/it]words:   9%|▉         | 28/320 [04:52<49:47, 10.23s/it]words:   9%|▉         | 29/320 [05:03<49:35, 10.23s/it]words:   9%|▉         | 30/320 [05:13<49:24, 10.22s/it]words:  10%|▉         | 31/320 [05:23<49:12, 10.22s/it]words:  10%|█         | 32/320 [05:33<49:01, 10.21s/it]words:  10%|█         | 33/320 [05:44<48:50, 10.21s/it]words:  11%|█         | 34/320 [05:54<48:40, 10.21s/it]words:  11%|█         | 35/320 [06:04<48:33, 10.22s/it]words:  11%|█▏        | 36/320 [06:14<48:23, 10.22s/it]words:  12%|█▏        | 37/320 [06:25<48:19, 10.25s/it]words:  12%|█▏        | 38/320 [06:35<48:06, 10.24s/it]words:  12%|█▏        | 39/320 [06:45<47:53, 10.23s/it]words:  12%|█▎        | 40/320 [06:55<47:41, 10.22s/it]words:  13%|█▎        | 41/320 [07:05<47:29, 10.21s/it]words:  13%|█▎        | 42/320 [07:16<47:18, 10.21s/it]words:  13%|█▎        | 43/320 [07:26<47:08, 10.21s/it]words:  14%|█▍        | 44/320 [07:36<47:06, 10.24s/it]words:  14%|█▍        | 45/320 [07:46<46:54, 10.24s/it]words:  14%|█▍        | 46/320 [07:57<46:45, 10.24s/it]words:  15%|█▍        | 47/320 [08:07<46:32, 10.23s/it]words:  15%|█▌        | 48/320 [08:17<46:23, 10.23s/it]words:  15%|█▌        | 49/320 [08:27<46:11, 10.23s/it]words:  16%|█▌        | 50/320 [08:37<46:00, 10.22s/it]words:  16%|█▌        | 51/320 [08:48<45:49, 10.22s/it]words:  16%|█▋        | 52/320 [08:58<45:38, 10.22s/it]words:  17%|█▋        | 53/320 [09:08<45:36, 10.25s/it]words:  17%|█▋        | 54/320 [09:18<45:24, 10.24s/it]words:  17%|█▋        | 55/320 [09:29<45:12, 10.23s/it]words:  18%|█▊        | 56/320 [09:39<45:19, 10.30s/it]words:  18%|█▊        | 57/320 [09:49<45:02, 10.28s/it]words:  18%|█▊        | 58/320 [09:59<44:47, 10.26s/it]words:  18%|█▊        | 59/320 [10:10<44:34, 10.25s/it]words:  19%|█▉        | 60/320 [10:20<44:22, 10.24s/it]words:  19%|█▉        | 61/320 [10:30<44:18, 10.26s/it]words:  19%|█▉        | 62/320 [10:40<44:04, 10.25s/it]words:  20%|█▉        | 63/320 [10:51<43:51, 10.24s/it]words:  20%|██        | 64/320 [11:01<43:41, 10.24s/it]slurmstepd: error: *** JOB 52473564 ON seasdgx104 CANCELLED AT 2020-04-12T07:07:16 ***
