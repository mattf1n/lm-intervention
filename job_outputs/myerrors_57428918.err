You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
  0%|          | 0/18 [00:00<?, ?it/s]  6%|▌         | 1/18 [00:33<09:22, 33.11s/it] 11%|█         | 2/18 [01:06<08:50, 33.17s/it] 17%|█▋        | 3/18 [01:39<08:17, 33.18s/it] 22%|██▏       | 4/18 [02:12<07:43, 33.13s/it] 28%|██▊       | 5/18 [02:45<07:11, 33.16s/it] 33%|███▎      | 6/18 [03:19<06:41, 33.44s/it] 39%|███▉      | 7/18 [03:53<06:06, 33.35s/it] 44%|████▍     | 8/18 [04:26<05:32, 33.29s/it] 50%|█████     | 9/18 [04:59<04:59, 33.25s/it] 56%|█████▌    | 10/18 [05:32<04:25, 33.19s/it] 61%|██████    | 11/18 [06:06<03:53, 33.39s/it] 67%|██████▋   | 12/18 [06:40<03:21, 33.54s/it] 72%|███████▏  | 13/18 [07:13<02:47, 33.58s/it] 78%|███████▊  | 14/18 [07:46<02:13, 33.37s/it] 83%|████████▎ | 15/18 [08:19<01:39, 33.22s/it] 89%|████████▉ | 16/18 [08:52<01:05, 33.00s/it] 94%|█████████▍| 17/18 [09:25<00:33, 33.02s/it]100%|██████████| 18/18 [09:57<00:00, 32.86s/it]100%|██████████| 18/18 [09:57<00:00, 33.20s/it]
