You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
  0%|          | 0/18 [00:00<?, ?it/s]  6%|▌         | 1/18 [00:32<09:15, 32.70s/it] 11%|█         | 2/18 [01:05<08:44, 32.75s/it] 17%|█▋        | 3/18 [01:38<08:11, 32.76s/it] 22%|██▏       | 4/18 [02:10<07:37, 32.70s/it] 28%|██▊       | 5/18 [02:43<07:05, 32.72s/it] 33%|███▎      | 6/18 [03:16<06:32, 32.73s/it] 39%|███▉      | 7/18 [03:49<06:01, 32.84s/it] 44%|████▍     | 8/18 [04:22<05:27, 32.79s/it] 50%|█████     | 9/18 [04:54<04:54, 32.75s/it] 56%|█████▌    | 10/18 [05:27<04:22, 32.86s/it] 61%|██████    | 11/18 [06:01<03:51, 33.06s/it] 67%|██████▋   | 12/18 [06:35<03:19, 33.21s/it] 72%|███████▏  | 13/18 [07:08<02:46, 33.24s/it] 78%|███████▊  | 14/18 [07:40<02:12, 33.02s/it] 83%|████████▎ | 15/18 [08:13<01:38, 32.89s/it] 89%|████████▉ | 16/18 [08:45<01:05, 32.65s/it] 94%|█████████▍| 17/18 [09:18<00:32, 32.70s/it]100%|██████████| 18/18 [09:50<00:00, 32.54s/it]100%|██████████| 18/18 [09:50<00:00, 32.81s/it]
