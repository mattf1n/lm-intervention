You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
  0%|          | 0/19 [00:00<?, ?it/s]  5%|▌         | 1/19 [00:48<14:40, 48.89s/it] 11%|█         | 2/19 [01:37<13:51, 48.94s/it] 16%|█▌        | 3/19 [02:26<13:03, 48.94s/it] 21%|██        | 4/19 [03:15<12:13, 48.89s/it] 26%|██▋       | 5/19 [04:04<11:25, 48.98s/it] 32%|███▏      | 6/19 [04:53<10:36, 48.98s/it] 37%|███▋      | 7/19 [05:42<09:48, 49.00s/it] 42%|████▏     | 8/19 [06:31<08:59, 49.03s/it] 47%|████▋     | 9/19 [07:20<08:09, 48.93s/it] 53%|█████▎    | 10/19 [08:09<07:19, 48.84s/it] 58%|█████▊    | 11/19 [08:58<06:30, 48.83s/it] 63%|██████▎   | 12/19 [09:46<05:41, 48.80s/it] 68%|██████▊   | 13/19 [10:36<04:53, 48.96s/it] 74%|███████▎  | 14/19 [11:25<04:04, 48.97s/it] 79%|███████▉  | 15/19 [12:14<03:15, 48.94s/it] 84%|████████▍ | 16/19 [13:03<02:26, 48.96s/it] 89%|████████▉ | 17/19 [13:51<01:37, 48.86s/it] 95%|█████████▍| 18/19 [14:40<00:48, 48.76s/it]100%|██████████| 19/19 [15:29<00:00, 48.82s/it]100%|██████████| 19/19 [15:29<00:00, 48.90s/it]
