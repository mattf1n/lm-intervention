You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
  0%|          | 0/19 [00:00<?, ?it/s]  5%|▌         | 1/19 [00:48<14:37, 48.77s/it] 11%|█         | 2/19 [01:37<13:50, 48.83s/it] 16%|█▌        | 3/19 [02:26<13:01, 48.86s/it] 21%|██        | 4/19 [03:15<12:12, 48.82s/it] 26%|██▋       | 5/19 [04:04<11:24, 48.92s/it] 32%|███▏      | 6/19 [04:53<10:35, 48.92s/it] 37%|███▋      | 7/19 [05:42<09:47, 48.94s/it] 42%|████▏     | 8/19 [06:31<08:58, 48.94s/it] 47%|████▋     | 9/19 [07:20<08:08, 48.88s/it] 53%|█████▎    | 10/19 [08:08<07:19, 48.86s/it] 58%|█████▊    | 11/19 [08:57<06:31, 48.88s/it] 63%|██████▎   | 12/19 [09:47<05:42, 49.00s/it] 68%|██████▊   | 13/19 [10:36<04:54, 49.16s/it] 74%|███████▎  | 14/19 [11:25<04:05, 49.17s/it] 79%|███████▉  | 15/19 [12:14<03:16, 49.05s/it] 84%|████████▍ | 16/19 [13:03<02:27, 49.05s/it] 89%|████████▉ | 17/19 [13:52<01:37, 48.83s/it] 95%|█████████▍| 18/19 [14:40<00:48, 48.71s/it]100%|██████████| 19/19 [15:29<00:00, 48.74s/it]100%|██████████| 19/19 [15:29<00:00, 48.91s/it]
